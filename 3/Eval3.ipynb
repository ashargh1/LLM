{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f199ab1-773f-4cf4-a22c-23788e6ceaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: This code is used to obtain the data needed for plotting Figure 4(b) of the mansuciprt in the next part. Also, this \n",
    "#Could can be simplified to obtain the data need for plotting Figure 2(b).\n",
    "\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "#fig, ax = plt.subplots()\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim, autograd\n",
    "from math import pi\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "np.random.seed(123456)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "class Unit3(nn.Module):\n",
    "    def __init__(self, in_N, out_N,actf):\n",
    "        super(Unit3, self).__init__()\n",
    "        self.in_N = in_N\n",
    "        self.out_N = out_N\n",
    "        self.actf = actf\n",
    "        self.L = nn.Linear(in_N, out_N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        actf=self.actf\n",
    "        x1 = self.L(x)\n",
    "        if actf==0:\n",
    "            x2 = torch.tanh(x1)\n",
    "        elif actf==1:\n",
    "            x2 = torch.sigmoid(x1) \n",
    "        elif actf==2:\n",
    "            x2 = torch.relu(x1)\n",
    "        elif actf==3:\n",
    "            x2 = torch.selu(x1)\n",
    "        elif actf==4:\n",
    "            x2 = F.softmax(x1, dim=1)\n",
    "        return x2\n",
    "    \n",
    "class NN3(nn.Module):\n",
    "    def __init__(self, in_N, width1, depth1,width2, depth2,out_N,bn,dp,dprate,actf):\n",
    "        super(NN3, self).__init__()\n",
    "        self.width1 = width1\n",
    "        self.width2 = width2\n",
    "        self.depth1 = depth1\n",
    "        self.depth2 = depth2\n",
    "        self.bn = bn\n",
    "        self.dp = dp\n",
    "        self.dprate = dprate\n",
    "        self.actf = actf\n",
    "        self.in_N = in_N\n",
    "        self.out_N = out_N\n",
    "        self.stack = nn.ModuleList()\n",
    "        self.stack.append(Unit3(in_N, width1[0],actf))\n",
    "        if bn==1:\n",
    "            self.stack.append(nn.BatchNorm1d(width1[0]))\n",
    "        for i in range(1,depth1):\n",
    "            self.stack.append(Unit3(width1[i-1], width1[i],actf))\n",
    "        \n",
    "        if dp==1:\n",
    "            self.stack.append(nn.Dropout(p=dprate))\n",
    "        if depth2==1:\n",
    "            self.stack.append(Unit3(width1[i], width2[0],1)) \n",
    "        else:\n",
    "            self.stack.append(Unit3(width1[i], width2[0],actf))    \n",
    "            for i in range(1,depth2-1):\n",
    "                self.stack.append(Unit3(width2[i-1], width2[i],actf))\n",
    "            self.stack.append(Unit3(width2[depth2-2], width2[depth2-1],4)) \n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.stack)):\n",
    "            x = self.stack[i](x)\n",
    "        return x\n",
    "\n",
    "activation=0\n",
    "dropout=1\n",
    "dropout_rate=0.29791\n",
    "normalization=1\n",
    "batch_size=1000\n",
    "layers1=10\n",
    "layers2=1\n",
    "neurons=86\n",
    "learning_rate=0.00065\n",
    "L1=[neurons]*layers1\n",
    "L2=[neurons]*layers2+[8]\n",
    "model_h = NN3(35,L1,layers1,L2,layers2+1, 8,normalization,dropout,dropout_rate,0)    \n",
    "load=1\n",
    "PATH=\"checkpoint/model-1406.pt\"\n",
    "if load==1:\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model_h.load_state_dict(checkpoint['model_h_state_dict'])\n",
    "    optimizer2 = optim.AdamW([{'params': model_h.parameters()}], lr=learning_rate) \n",
    "    optimizer2.load_state_dict(checkpoint['optimizer2_state_dict'])    \n",
    "model_h.eval()\n",
    "\n",
    "x=np.load('xlo_test.npy')\n",
    "y=np.load('ylo_test.npy')\n",
    "\n",
    "\n",
    "pred_2h_star_test= model_h(torch.from_numpy(x).float())\n",
    "B=pred_2h_star_test.detach().numpy()[:, :]\n",
    "A=np.copy(y)\n",
    "\n",
    "m=0\n",
    "for i in range(B.shape[0]):\n",
    "    n=0\n",
    "    for j in range(B.shape[1]):\n",
    "        if np.abs(B[i,j]-A[i,j])>0.07: #Threshold\n",
    "            n=1\n",
    "    if n==0:\n",
    "        m=m+1\n",
    "print('The accuracy corresponding to the chosen absolute deviation tolerance per phase=',m/B.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASharghEnv",
   "language": "python",
   "name": "asharghenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
